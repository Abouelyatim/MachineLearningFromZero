{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP06 : Naïve Bayes\n",
    "\n",
    "Tout le monde connait le théorème de Bayes pour calculer la probabilité conditionnelle d'un évennement $A$ sachant un autre $B$: \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce théorème sur un problème d'appentissage automatique, l'idée est simple ; Etant donné une caractéristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'équation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ postérieure \n",
    "- $P(y=c)$ antérieure\n",
    "- $P(f|y=c)$ vraisemblance\n",
    "- $P(f)$ évidence \n",
    "\n",
    "Ici, on estime la probablité d'une classe $c$ sachant une caractéristique $f$ en utilisant des données d'entrainement. Maintenant, on veut estimer la probabilité d'une classe $c$ sachant un vecteur de caractéristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donnée plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilité maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'évidence pour cacher le crime : $P(f)$ ne dépend pas de $c_k$ et elle est postive, donc ça ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properiété naïve (d'où vient le nom Naive Bayes) : on suppose l'indépendence conditionnelle entre les caractéristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour éviter la disparition de la probabilité (multiplication et représentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "\n",
    "## Avantages \n",
    "\n",
    "Les classifieurs naïfs bayésiens, malgré leur simplicité, ont des points forts:\n",
    "- Ils ont besoin d'une petite quantité de données d'entrainement.\n",
    "- Ils sont très rapides par rapport aux autres classifieurs.\n",
    "- Ils donnent de bons résultats dans le cas de filtrage du courrier indésirable et de classification de documents.\n",
    "\n",
    "## Limites\n",
    "Les classifieurs naïfs bayésiens certes sont populaires à cause de leur simplicité. Mais, une telle simplicité vient avec un coût [référence: Spiderman].\n",
    "- Les probabilités obtenues en utilisant ces classifieurs ne doivent pas être prises au sérieux.\n",
    "- S'il existe une grande corrélation entre les caractéristiques, ils vont donner une mauvaise performance.\n",
    "- Dans le cas des caractéristiques continues (prix, surface, etc.), les données doivent suivre la loi normale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Implémentation\n",
    "\n",
    "Pour estimer la vraisemblance, il y a plusieurs modèles (lois):\n",
    "- Loi multinomiale : pour les caracétristiques nominales\n",
    "- Loi de Bernoulli : lorsqu'on est interressé par l'apparence d'une caractéristique ou non (binaire)\n",
    "- loi normale : pour les caractéristiques numériques\n",
    "\n",
    "Dans ce TP, on va implémenter Naive Bayes pour les caractéristiques nominales (loi multinomiale)\n",
    "\n",
    "### I-1- Les données pour les tests unitaires\n",
    "Ici, on va utiliser le dataset \"jouer\" contenant des caractéristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "jouer = pd.read_csv(\"datasets/jouer.csv\")\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premières colonnes \n",
    "Y_jouer = jouer.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Estimation de la probabilité antérieure\n",
    "Etant donné le vecteur de sortie $Y$, on doit calculer la probabilité de chaque classe (différentes valeurs de $Y$)\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "La fonction doit retourner un dictionnaire où la clé est le nom de la classe et la valeur est sa probabilité. Voici, un exemple d'un dictionnaire dans Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Iris-setosa)= 0.5\n",
      "P(Iris-versicolor)= 0.33\n",
      "P(Iris-virginica)= 0.67\n"
     ]
    }
   ],
   "source": [
    "# Exemple de dictionnaire dans Python \n",
    "d = {}\n",
    "d[\"Iris-setosa\"] = 0.5\n",
    "d[\"Iris-versicolor\"] = 0.33\n",
    "d[\"Iris-virginica\"] = 0.67\n",
    "\n",
    "for c in d: \n",
    "    print(\"P(\" + c + \")= \" + str(d[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.35714285714285715, 'oui': 0.6428571428571429}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser la fonction \n",
    "def P_c(Y): \n",
    "    vals = np.unique(Y)\n",
    "    resultat = {}\n",
    "    for val in vals:\n",
    "        resultat.update({val: np.count_nonzero(Y==val)/Y.size})\n",
    "    return resultat\n",
    "\n",
    "\n",
    "        \n",
    "# Résultat: {'non': 0.35714285714285715, 'oui': 0.6428571428571429}\n",
    "P_c(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Entrainement  (loi multinomiale)\n",
    "\n",
    "Notre modèle (notons le par $\\theta_{f_j,C}$) doit garder le nombre des différentes valeurs dans une caractéristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "\n",
    "Donc, étant donné un vecteur d'une caractéristique $A$ et un autre des $Y$ respectives, la fonction d'entrainement doit retourner un dictionnaire (notre théta) : \n",
    "- la clé est une valeur $a_v$ de $A$ \n",
    "- la valeur est un autre dictionnaire : \n",
    "   - il doit contenir une clé \"_total_\" dont la valeur est le nombre d'occurence de $a_v$ dans $A$ \n",
    "   - la clé est la classe $c_k$ de $Y$\n",
    "   - la valeur est le nombre d'occurence de $a_v$ respectives à $c_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensoleile': {'__total__': 5, 'non': 3, 'oui': 2},\n",
       " 'nuageux': {'__total__': 4, 'non': 0, 'oui': 4},\n",
       " 'pluvieux': {'__total__': 5, 'non': 2, 'oui': 3}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser cette fonction \n",
    "# Elle génère théta pour une seule caractéristique\n",
    "def entrainer_multi_1(A, Y): \n",
    "    resultat = {}\n",
    "    A_vals = np.unique(A)\n",
    "    Y_vals = np.unique(Y)\n",
    "    for val in A_vals:\n",
    "        resultat[val] = {}\n",
    "        resultat[val]['__total__'] = np.count_nonzero(A==val)\n",
    "        for Y_val in Y_vals:\n",
    "            resultat[val][Y_val] = (A==val)[Y==Y_val].sum()\n",
    "    return resultat\n",
    "\n",
    "# Résultat \n",
    "# {'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
    "# 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
    "# 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}\n",
    "Theta_jouer_temps = entrainer_multi_1(X_jouer[:, 0], Y_jouer)\n",
    "\n",
    "Theta_jouer_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ensoleile': {'__total__': 5, 'non': 3, 'oui': 2},\n",
       "  'nuageux': {'__total__': 4, 'non': 0, 'oui': 4},\n",
       "  'pluvieux': {'__total__': 5, 'non': 2, 'oui': 3}},\n",
       " {'chaude': {'__total__': 4, 'non': 2, 'oui': 2},\n",
       "  'douce': {'__total__': 6, 'non': 2, 'oui': 4},\n",
       "  'fraiche': {'__total__': 4, 'non': 1, 'oui': 3}},\n",
       " {'haute': {'__total__': 7, 'non': 4, 'oui': 3},\n",
       "  'normale': {'__total__': 7, 'non': 1, 'oui': 6}},\n",
       " {'non': {'__total__': 8, 'non': 2, 'oui': 6},\n",
       "  'oui': {'__total__': 6, 'non': 3, 'oui': 3}},\n",
       " {'non': 0.35714285714285715, 'oui': 0.6428571428571429}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Théta sur plusieurs caractéristiques\n",
    "# Rien à programmer ici\n",
    "# Notre théta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient le théta de la caractéristique respective à la colonne de X\n",
    "# On ajoute les probabilités antérieures des classes à la fin de résultat\n",
    "def entrainer_multi(X, Y): \n",
    "    resultat = []\n",
    "    for i in range(X.shape[1]): \n",
    "        resultat.append(entrainer_multi_1(X[:, i], Y))\n",
    "    resultat.append(P_c(Y))\n",
    "    return resultat\n",
    "\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Estimation de la probabilité de vraissemblance (loi multinomiale)\n",
    "L'équation pour estimer la vraisemblance \n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "Si, dans le dataset de test, on veut calculer la probabilité d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donnée, on aura une probabilité nulle. Ici, on doit appliquer une fonction de lissage qui donne une petite probabilité aux données non vues dans l'entrainnement. Le lissage qu'on va utiliser est celui de Lidstone. Lorsque $\\alpha = 1$ on l'appelle lissage de Laplace.\n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "Où: \n",
    "- $\\alpha$ est une valeur donnée \n",
    "- $V$ est l'ensemble des différentes valeurs de $f_j$ (le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.08333333333333333)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter cette fonction\n",
    "def P_vraiss_multi(Theta_j, v, c, alpha=1.): \n",
    "    len_V = len(Theta_j) # La taille du vocabulaire\n",
    "    nbr_c = 0\n",
    "    for i in Theta_j.keys():\n",
    "        nbr_c += Theta_j[i][c]\n",
    "    if v in Theta_j.keys():\n",
    "        return (Theta_j[v][c] + alpha) / (nbr_c + alpha * len_V)\n",
    "    else:\n",
    "        return alpha / (nbr_c + alpha*len_V)\n",
    "\n",
    "# La probabilité de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilité de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "\n",
    "\n",
    "P_vraiss_multi(Theta_jouer_temps, \"pluvieux\", \"oui\"), P_vraiss_multi(Theta_jouer_temps, \"neigeux\", \"oui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Prédiction de la classe (loi multinomiale)\n",
    "Revenons maintenant à notre équation de prédiction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Ici, vous devez prédire un seule échantillon $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('oui', -4.102643365036796), ('oui', -3.6608106127577567))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter ce code\n",
    "# Pour récupérer le théta de la caractéristique n°0 : Theta[0]\n",
    "# anter est un booléen, si il est False, on ne compte pas la probabilité antérieure P(y = c_k)\n",
    "def predire(x, Theta, alpha=1., anter=True): \n",
    "    c_opt = \"\" # la classe optimale\n",
    "    p_c = Theta[-1] #les classes et leurs probabilités antérieures\n",
    "    if not anter: # si on ne veut pas ajouter les probabiliés antérieures\n",
    "        p_c = dict.fromkeys(p_c, 1.) # on définit le tous en 1; log(1) = 0\n",
    "    max_log_p = np.NINF # - infinity \n",
    "    # compléter ici\n",
    "    \n",
    "    for c in p_c.keys():\n",
    "        max_ = np.log(p_c[c])\n",
    "        i = 0\n",
    "        for v in Theta[:len(Theta) - 1]:\n",
    "            max_ += np.log(P_vraiss_multi(v,x[i],c))\n",
    "            i += 1\n",
    "        if max_log_p < max_:\n",
    "            max_log_p = max_\n",
    "            c_opt = c\n",
    "    return c_opt, max_log_p\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# Résultat: (('oui', -4.102643365036796), ('oui', -3.6608106127577567))\n",
    "predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer), predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer, anter=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien à programmer ici, il y a une petite analyse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            c, p = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(p)\n",
    "            else:\n",
    "                Y_pred.append(c)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va entrainer un modèle en utilisant notre imlémentation avec et sans probabilité antérieure. \n",
    "Normalement, on doit tester sur des données non vues (des données qu'on n'a pas utilisé pour l'entrainement). Mais, ici, on va tester sur les mêmes données d'entrainement afin de savoir si le modèle a bien représenté ce dataset ou non (calculer l'erreur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle avec probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      1.00      0.89         4\n",
      "         oui       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.90      0.95      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Notre modèle sans probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      0.67      0.73         6\n",
      "         oui       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.79      0.77      0.78        14\n",
      "weighted avg       0.79      0.79      0.78        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False)\n",
    "\n",
    "# Ici, ce n'ai pas la peine d'exécuter plusieurs fois\n",
    "# puisque le résultat sera le même \n",
    "\n",
    "# Le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Notre modèle avec probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_ant, Y_jouer))\n",
    "\n",
    "print(\"Notre modèle sans probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_sans_ant, Y_jouer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats** :\n",
    "\n",
    "* modèle avec probabilité antérieure:\n",
    "\n",
    "    -l'utilisation de la  probabilité antérieure \"P_c\" ce qui est donnée par la proportion de contribution de la class dans le dataset,ce modele utilise cette probabilité dans le clacule de la probabilité postérieure,donc le classification est bonne,avec l'accuracy==93%\n",
    "    \n",
    "\n",
    "* modèle avec probabilité antérieure:\n",
    "\n",
    "    -ce modele considère que tout les données sont équiprobable,donc il ya une marge d'erreur qui affecte la resultat,c'est pour ca l'accuracy de ce modele est mauvaise ==79%\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Détection de spam \n",
    "\n",
    "Ici, on va essayer d'appliquer l'apprentissage automatique sur la détection de spam. \n",
    "Chaque message dans le dataset est représenté en utilisant un modèle \"Sac à mots\" (BoW : Bag of Words).\n",
    "Dans l'entrainement, on récupère les différents mots qui s'apparaissent dans les messages. \n",
    "Chaque mot va être considéré comme une caractéristique. \n",
    "Donc, pour chaque message, la valeur de la caractéristique est la fréquence de son mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caractéristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre implémentation n'est pas adéquate pour la nature de ce problème. \n",
    "Dans Scikit-learn, le [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire à notre implémentation. \n",
    "L'algorithme adéquat pour ce type de problème est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "Le dataset utilisé est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Les algorithmes comparés :\n",
    "- Naive Bayes\n",
    "- Arbre de décision\n",
    "- Regression logistique \n",
    "\n",
    "### II-1- Préparation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte classe\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv(\"datasets/spam.csv\", encoding=\"latin-1\")\n",
    "messages = messages.rename(columns={\"v1\": \"classe\", \"v2\": \"texte\"})\n",
    "messages = messages.filter([\"texte\", \"classe\"])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Entrainement et test des modèles sur plusieurs exécutions \n",
    "\n",
    "Afin de satisfaire un étudiant qui réclame toujours sur le manque des données, nous avons décidé de comparer les algorithmes sur plusieurs excécutions (runs). \n",
    "\n",
    "**Rien à analyser ici**\n",
    "\n",
    "**P.S.** timeit.default_timer() est dépendante du système d'exploitation. Aussi, elle peut être affectée par d'autre processus en parallèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'naive_bayes': [0.0150867,\n",
       "  0.019919299999999973,\n",
       "  0.014701999999999993,\n",
       "  0.019833300000000165,\n",
       "  0.022902300000000153,\n",
       "  0.0225679999999997,\n",
       "  0.014643500000000031],\n",
       " 'arbre_decision': [0.1839893,\n",
       "  0.1987276,\n",
       "  0.18311880000000014,\n",
       "  0.2097595000000001,\n",
       "  0.17678549999999982,\n",
       "  0.20698499999999997,\n",
       "  0.19385370000000002],\n",
       " 'reg_log': [0.1553117,\n",
       "  0.13496470000000016,\n",
       "  0.09319739999999999,\n",
       "  0.10481469999999993,\n",
       "  0.10495239999999972,\n",
       "  0.10895089999999996,\n",
       "  0.11346079999999992]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "NBR_RUN = 7\n",
    "\n",
    "temps_train = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "temps_test = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "perf = {\n",
    "    \"naive_bayes_P\" : [],\n",
    "    \"arbre_decision_P\": [],\n",
    "    \"reg_log_P\": [], \n",
    "    \"naive_bayes_R\" : [],\n",
    "    \"arbre_decision_R\": [],\n",
    "    \"reg_log_R\": []\n",
    "}\n",
    "\n",
    "\n",
    "for run in range(NBR_RUN): \n",
    "    # prétaitement des données\n",
    "    msg_train, msg_test, Y_train, Y_test = train_test_split(messages[\"texte\"],messages[\"classe\"],test_size=0.2)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(msg_train)\n",
    "    X_test = count_vectorizer.transform(msg_test)\n",
    "    \n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    \n",
    "    #entrainement Naive Bayes\n",
    "    naive_bayes = MultinomialNB()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    temps_train[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement CART\n",
    "    arbre_decision = DecisionTreeClassifier()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    arbre_decision.fit(X_train, Y_train)\n",
    "    temps_train[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement Régression logitique\n",
    "    reg_log = LogisticRegression(solver=\"lbfgs\") #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    temps_debut = timeit.default_timer()\n",
    "    reg_log.fit(X_train, Y_train)\n",
    "    temps_train[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    \n",
    "    #test Naive Bayes\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_naive_bayes = naive_bayes.predict(X_test)\n",
    "    temps_test[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    \n",
    "    #test CART\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_arbre_decision = arbre_decision.predict(X_test)\n",
    "    temps_test[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #test Régression logitique\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_reg_log = reg_log.predict(X_test)\n",
    "    temps_test[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, on va considérer une classification binaire avec une seule classe \"spam\" \n",
    "    # On ne juge pas le classifieur sur sa capacité de détecter les non spams\n",
    "    \n",
    "    perf[\"naive_bayes_P\"].append(precision_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_P\"].append(precision_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_P\"].append(precision_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    perf[\"naive_bayes_R\"].append(recall_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_R\"].append(recall_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_R\"].append(recall_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    \n",
    "\n",
    "temps_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Analyse du temps d'apprentissage \n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le même dataset d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015087</td>\n",
       "      <td>0.183989</td>\n",
       "      <td>0.155312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.198728</td>\n",
       "      <td>0.134965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.183119</td>\n",
       "      <td>0.093197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019833</td>\n",
       "      <td>0.209760</td>\n",
       "      <td>0.104815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022902</td>\n",
       "      <td>0.176785</td>\n",
       "      <td>0.104952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022568</td>\n",
       "      <td>0.206985</td>\n",
       "      <td>0.108951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014644</td>\n",
       "      <td>0.193854</td>\n",
       "      <td>0.113461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.015087        0.183989  0.155312\n",
       "1     0.019919        0.198728  0.134965\n",
       "2     0.014702        0.183119  0.093197\n",
       "3     0.019833        0.209760  0.104815\n",
       "4     0.022902        0.176785  0.104952\n",
       "5     0.022568        0.206985  0.108951\n",
       "6     0.014644        0.193854  0.113461"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**:\n",
    "\n",
    "    -la procedure de la construction de l'arbre de decision est un peu complexe, cela ce qui justifie la duré d'entrainement\n",
    "\n",
    "    - la methode de Reg log utilise la fonction de decente de gradient pour minimiser l'erreur, ce qui peut prendre du temps, mais elle est toujour moins complexe que les arbres de decision, et elle present une duré d'entrainement moyenne par rapport aux autre algo.\n",
    "    \n",
    "    -la methode de Naive Bayes est très rapide dans la construction du modele, ce qui justifie la duré d'entrianement de cette derniere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Analyse du temps de test \n",
    "\n",
    "Combien de temps chaque algorithme prend pour prédir les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.000592        0.000742  0.000223\n",
       "1     0.001192        0.001320  0.000452\n",
       "2     0.000494        0.000756  0.000226\n",
       "3     0.000636        0.001012  0.000347\n",
       "4     0.000653        0.000843  0.000227\n",
       "5     0.000723        0.001114  0.000357\n",
       "6     0.000567        0.000914  0.000279"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**:\n",
    "\n",
    "    - les arbres de decision ont une structure hiérarchique et peuvent être profondue, ce qui affecte le processus de prediction,  c'est pour ca les arbres de decision sont un peu lentes par rapport aux autre algo.\n",
    "\n",
    "    - Les deux autres models s'approchent l'un de l'autre en temps de prediction et le temps dépendent principalement du temps de calcule et opérations efectuer (Multuplication, division ...etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Analyse de la performance \n",
    "\n",
    "Ici, on compare les modèles en se basant sur leurs capacités à détecter le spam. \n",
    "On va utiliser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbre_decision_P</th>\n",
       "      <th>naive_bayes_P</th>\n",
       "      <th>reg_log_P</th>\n",
       "      <th>arbre_decision_R</th>\n",
       "      <th>naive_bayes_R</th>\n",
       "      <th>reg_log_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.946565</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.931298</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.839161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.902256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.827815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.900763</td>\n",
       "      <td>0.969925</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.902985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.965753</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arbre_decision_P  naive_bayes_P  reg_log_P  arbre_decision_R  \\\n",
       "0          0.946565       0.970149   0.992248          0.849315   \n",
       "1          0.931298       0.992126   0.991736          0.853147   \n",
       "2          0.913333       0.979592   0.971429          0.895425   \n",
       "3          0.893939       0.960938   0.975610          0.887218   \n",
       "4          0.909091       0.963504   1.000000          0.860927   \n",
       "5          0.900763       0.969925   0.983740          0.880597   \n",
       "6          0.912162       0.965753   0.992366          0.894040   \n",
       "\n",
       "   naive_bayes_R  reg_log_R  \n",
       "0       0.890411   0.876712  \n",
       "1       0.881119   0.839161  \n",
       "2       0.941176   0.888889  \n",
       "3       0.924812   0.902256  \n",
       "4       0.874172   0.827815  \n",
       "5       0.962687   0.902985  \n",
       "6       0.933775   0.860927  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arbre_decision_P    0.915307\n",
       "naive_bayes_P       0.971712\n",
       "reg_log_P           0.986733\n",
       "arbre_decision_R    0.874381\n",
       "naive_bayes_R       0.915450\n",
       "reg_log_R           0.871249\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**:\n",
    "\n",
    "* **Naive Bayes**:\n",
    "    - avec un rappel 91%, ce modele a le meilleur rappel par rapport aux autre methodes avec une très bonne precision (très proche à celle de Reg Log) ,<br/> un rappel de 91% veut dire que, dans 91% des cas, ce modele il a classifier correctement les messages comme SPAM avec une precision 97% .\n",
    "\n",
    "* **Regression Logistique**:\n",
    "    - ce modele present un rappel de 87% avec la meilleur precision (98%) par rapport aux autre modeles, ce modele predire correctement 87% des messages comme SPAM.\n",
    "\n",
    "* **Arbre de decision**:\n",
    "    - ce modele present la plus faible resultat par rapport aux autre modele, et ca du à sa vulnérabilité aux suraperentissage, ce qui peut produire des erreurs et par conséquent affecter la precision et l'accuracy du modele,<br> avec un rappel 87% et une precision 91% on peut dire que les arbres de decision ne sont pas convenable à notre cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
